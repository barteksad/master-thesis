% Niniejszy plik stanowi przykład formatowania pracy magisterskiej na
% Wydziale MIM UW.  Szkielet użytych poleceń można wykorzystywać do
% woli, np. formatujac wlasna prace.
%
% Zawartosc merytoryczna stanowi oryginalnosiagniecie
% naukowosciowe Marcina Wolinskiego.  Wszelkie prawa zastrzeżone.
%
% Copyright (c) 2001 by Marcin Woliński <M.Wolinski@gust.org.pl>
% Poprawki spowodowane zmianami przepisów - Marcin Szczuka, 1.10.2004
% Poprawki spowodowane zmianami przepisow i ujednolicenie 
% - Seweryn Karłowicz, 05.05.2006
% Dodanie wielu autorów i tłumaczenia na angielski - Kuba Pochrybniak, 29.11.2016

% dodaj opcję [licencjacka] dla pracy licencjackiej
% dodaj opcję [en] dla wersji angielskiej (mogą być obie: [licencjacka,en])
\documentclass[licencjacka,en]{pracamgr}

\usepackage{times}
% \usepackage[marginparwidth=75pt]{geometry}
% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}

% additional packages
\usepackage{graphicx}
\usepackage{xspace}
\usepackage[capitalise]{cleveref}
\usepackage{wrapfig}
\usepackage{multirow}
\usepackage{array}
\usepackage{booktabs}
\usepackage{tabularray}
\usepackage{amsmath}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{subcaption}
\usepackage{natbib}  % or \usepackage{biblatex} if you prefer biblatex
\bibliographystyle{plainnat}  % or another style of your choice

\usepackage{xargs}                      % Use more than one optional parameter in a new commands
\usepackage[pdftex,dvipsnames]{xcolor}  % Coloured text etc.
\usepackage[colorinlistoftodos,prependcaption,textsize=tiny]{todonotes}
\newcommandx{\unsure}[2][1=]{\todo[linecolor=red,backgroundcolor=red!25,bordercolor=red,#1]{#2}}
\newcommandx{\change}[2][1=]{\todo[linecolor=blue,backgroundcolor=blue!25,bordercolor=blue,#1]{#2}}
\newcommandx{\info}[2][1=]{\todo[linecolor=OliveGreen,backgroundcolor=OliveGreen!25,bordercolor=OliveGreen,#1]{#2}}
\newcommandx{\improvement}[2][1=]{\todo[linecolor=Plum,backgroundcolor=Plum!25,bordercolor=Plum,#1]{#2}}
\newcommandx{\thiswillnotshow}[2][1=]{\todo[disable,#1]{#2}}

% Dane magistranta:
\autor{Bartłomiej Sadlej}{429589}

\title{Title in English}
\titlepl{Tytuł po polsku}


\kierunek{Machine Learning}

% informatyka - nie okreslamy zakresu (opcja zakomentowana)
% matematyka - zakres moze pozostac nieokreslony,
% a jesli ma byc okreslony dla pracy mgr,
% to przyjmuje jedna z wartosci:
% {metod matematycznych w finansach}
% {metod matematycznych w ubezpieczeniach}
% {matematyki stosowanej}
% {nauczania matematyki}
% Dla pracy licencjackiej mamy natomiast
% mozliwosc wpisania takiej wartosci zakresu:
% {Jednoczesnych Studiow Ekonomiczno--Matematycznych}

% \zakres{Tu wpisac, jesli trzeba, jedna z opcji podanych wyzej}

% Praca wykonana pod kierunkiem:
% (podać tytuł/stopień imię i nazwisko opiekuna
% Instytut
% ew. Wydział ew. Uczelnia (jeżeli nie MIM UW))
\opiekun{prof. Przemysław Biecek\\
  Wydział Matematyki Informatyki i Mechaniki\\
  }

% miesiąc i~rok:
\date{May 2017}

%Podać dziedzinę wg klasyfikacji Socrates-Erasmus:
\dziedzina{ 
%11.0 Matematyka, Informatyka:\\ 
%11.1 Matematyka\\ 
% 11.2 Statystyka\\ 
%11.3 Informatyka\\ 
11.4 Sztuczna inteligencja\\ 
%11.5 Nauki aktuarialne\\
%11.9 Inne nauki matematyczne i informatyczne
}

%Klasyfikacja tematyczna wedlug AMS (matematyka) lub ACM (informatyka)
\klasyfikacja{D. Software\\
  D.127. Blabalgorithms\\
  D.127.6. Numerical blabalysis}

% Słowa kluczowe:
\keywords{blabaliza różnicowa, fetory $\sigma$-$\rho$, fooizm,
  blarbarucja, blaba, fetoryka, baleronik}

% Tu jest dobre miejsce na Twoje własne makra i~środowiska:
\newtheorem{defi}{Definicja}[section]

% koniec definicji

\begin{document}
\maketitle

%tu idzie streszczenie na strone poczatkowa
\begin{abstract}
  
  This thesis explores the generation of invariant sets of images with respect to specific objective functions, particularly in the context of Computer Vision model interpretation. While numerous approaches exist for explaining model behavior through concepts discovery or interpretable approximation, we focus on a novel problem: generating sets of images that yield identical predictions under a given objective function, thereby establishing an equivalence relation.

  Unlike adversarial example optimization, our research aims to develop an algorithm that effectively samples meaningful and diverse examples from these invariant sets, primarily leveraging Diffusion Models for image generation. Building upon recent advances in treating diffusion models as unsupervised priors for inverse problems, we extend existing frameworks to accommodate arbitrary objective functions defined in either image space or model neuron space.

  Our work addresses a significant gap in current research, which has focused primarily on analyzing training data and model behavior on known inputs. By diving into the unexplored domain of alternative inputs yielding identical outcomes, we contribute to a deeper understanding of model behavior and interpretation. We demonstrate the effectiveness of our approach through extensive experiments and provide insights into the practical applications of invariant set generation for model explanation.
\end{abstract}

\tableofcontents
%\listoffigures
%\listoftables

\chapter*{Wprowadzenie}
\addcontentsline{toc}{chapter}{Wprowadzenie}

Explaining Computer Vision models is an active area of research in machine learning due to many practical benefits that can be derived from understanding the model's behavior. Over time, diverse approaches have been proposed to find particular features responsible for the model's predictions or to approximate the model's behavior in a more interpretable way \todo{citations}.
In this work we focus on the problem of finding invariant sets of images that given any objective function result in the same prediction and constitute to a equivalence relation defined by this function and its range.
Our goal is not to optimize for adversarial examples \todo{citations}, rather to establish an algorithm which can effectivly sample meaningful and diverse examples from the invariant set.


% \cite{sobieski2024rethinkingvisualcounterfactualexplanations} 
\chapter{Related work}\label{r:related_work}

\section{Diverse posterior sampling for Inverse Problem}
In th past, a lot of work has been done on treating diffusion models as unsupervised priors to solve inverse problems for image restoration such as denoising, inpainting, super-resolution, and deblurring. \todo{add citations}
Those methods were commonly designed to find a single best solution for the given image.
More recently the focus has shifted towards producing a range of diverse and meaningful valid solutions for evey image \cite{cohen2024posteriorsamplingmeaningfuldiversity}.
\cite{chung2024diffusionposteriorsamplinggeneral} demonstrate that score-based diffusion models (SGMs) \cite{song2021scorebasedgenerativemodelingstochastic} can efficiently handle noisy nonlinear inverse problems.
In our setting, the mesurement almost never lives in the dimensionality of data, which is explored in \cite{batzolis2021conditionalimagegenerationscorebased} and provides systematic comparison of best ways of estimating the conditional score.

\section{Concepts, Spurious features and Clever Hans detections}
\cite{Lapuschkin_2019} develops an automatic pipeline to for exploring shortcuts and biases learned by the model which are commonly referred to as Clever Hans \cite{pfungst1911cleverHans}.
\cite{neuhaus2023spuriousfeatureslargescale} investigates how to automatically find spurious features in the training data.
Recently \cite{dreyer2025mechanisticunderstandingvalidationlarge} answers the question of what concepts were learned by the model and where in the training data they were present. 
While those works are great at analyzing the training data and model's behavior on it, we shift our focus on the unexplored area of other inputs which can lead to the same outcomes.

\section{Classifier guidance for conditional image generation}
Diffusion models allows for incorporating an additional guidance signal into the denoising process. Most common approaches are either to use classifier-free guidance \cite{ho2022classifierfreediffusionguidance} and guide the generation with class label or utilise text prompt \cite{rombach2022highresolutionimagesynthesislatent}.
Both approaches doesn't allow for incorporating new classes or text conditioning models at inference time either due to fixed network architecture or distributions mismatch.
One way to include any classifier as a guidance signal is to map the noised image to its clean version at each step through the reverse process\cite{jeanneret2022diffusionmodelscounterfactualexplanations} which is very computationally expensive or use Tweedie Formula \citep{robbins1992empirical, chung2022improving, weng2024fast} to estimate the final solution which although introducing some bias works well in practise but limits the number of optimization steps to the number of diffusion steps. 
\cite{augustin2024digindiffusionguidanceinvestigating} uses classifier guidance to optimize initial noise fed to the diffusion model and makes it computationally feasible due to adoption of Latent Diffusion Models and moving from pixel space to latent space.
We set out to further extend this framework to arbitrary objective function defined either in the image space or in the model neurons space.

\chapter{LOREM IPSUM}

\bibliography{references}

\end{document}


%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% coding: latin-2
%%% End:
